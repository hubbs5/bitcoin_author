What would happen if some minority of mining hash power and maybe a merchant or exchange decided to stay with, or move to, different consensus rules than everybody else?, Would there be two different flavors of Bitcoin? Would it cause massive disruptions to the Bitcoin economy?  Would your coins be safe?, (spoiler alert if you’re in a hurry: no, no, and yes), I’ll start with the assumption that there is a supermajority (two-thirds or more– comfortably over 50%) that wants one set of consensus rules, and a minority that wants another set of consensus rules. This analysis doesn’t work if there is an even split in opinion about the rules. I’m also assuming that there is a supermajority of both hash power and transaction creators (the ‘economic majority’) on the same side; the analysis is different if miners and exchanges/merchants/users disagree about what the rules should be., So, if there is a split in the block chain, with one branch supported by a supermajority of economic and hash power and another branch supported by a minority, the branches will quickly become unequal in length. Let’s work through a scenario where there is an 80/20 split in hash power and assuming minimal changes to the consensus rules on both branches:, The majority branch will generate a block every 12.5 minutes for 17.5 days, until difficulty adjusts and it goes back to producing a block every ten minutes., Miners on the minority branch will generate a block every 50 minutes for 70 days, until difficulty adjusts and that branch gets back to producing a block every ten minutes., Miners have to wait 100 blocks before they can spend their newly minted coins, and that’s the first disincentive for miners on the minority branch: they will have to wait about three and a half days before moving those new coins to somebody willing to exchange them for goods or services or another currency. Miners on the majority branch can trade their new coins after less than a day., The real question for the minority miners is will there be anybody willing to trade for those newly minted “minoritycoins”? And what is the price likely to be?, Trading safely on the minority branch is difficult, because right after the fork every transaction is valid on both branches since unspent coins before the fork are valid on both branches. “Pay me eleven minoritycoins” is the same as “Pay me eleven majoritycoins” if the transaction will be accepted into both branches. If minoritycoins are worth less than majoritycoins, then that is a big problem., And minoritycoins will be worth much less than majoritycoins, because they are worse in every practical way. They take longer to confirm, fewer people accept them for payment, they are protected by less hash power, and they are much more difficult to transact safely., That is the real disincentive for miners to stick with the minority branch; in the three days it takes for their newly minted minoritycoins to mature they are very likely to see the price they can get for those coins drop to almost zero., If the minority makes some big changes to the consensus rules, then they could sidestep a lot of these problems. Resetting the difficulty will fix the problem of very slow-to-confirm transactions. Changing the proof-of-work would eliminate the risk (which I haven’t talked about) of the majority miners dedicating some hash power to attack the minority branch., Changing the transaction format so transactions aren’t valid on both chains would eliminate the risk of accidentally sending eleven ‘majoritycoins’ (worth a few thousand dollars) to somebody when you really meant to send eleven ‘minoritycoins’ (which might be worth much less)., But either of the above would be a radical change– equivalent to creating an altcoin. There are plenty of those (even one that kinda-sorta ran the “start with the Bitcoin ledger” experiment), but their existence doesn’t disrupt the Bitcoin economy or pose any threat to people who hold Bitcoin., A minority branch would be even less of a threat.
Computer science has this thing called “big-O notation” (that’s O as in “oh”, not zero). It is a way of describing how algorithms behave as they’re given bigger problems to solve., During the Great Maximum Blocksize Debate, there have been repeated claims that “Bitcoin is not scalable, because it is O(n2).” N-squared scaling is not sustainable; double N, and you need four times the resources (memory or CPU)., At the Montreal Scaling Bitcoin conference, I had a chance to talk to a couple of those people and ask them what the heck they’re talking about. Turns out they’re talking about a few different things., Some of them are taking Metcalfe’s Law (“the value of a telecommunications network is proportional to the square of the number of connected users of the system (n2)” and applying it to Bitcoin transactions, assuming that if there are N people using Bitcoin they will collectively generate N2 transactions. That’s just silly, because even if everybody can theoretically transact with everybody else using Bitcoin, they won’t. I’ve transacted with probably under 100 other people or companies in the five years I’ve been using Bitcoin; the demand for transactions scales up linearly with the number of people using it., Then there’s the observation that, assuming growth in the number of transactions “n” over time, the entire transaction history grows O(n2) (see Patrick Strateman’s recent talk). Grow faster than CPU power or hard disk space and eventually nobody new will be able to validate the entire blockchain., If you start with the belief that you must validate the entire blockchain from the genesis block forward to be a True Bitcoiner, then this is a problem. But that’s an unnecessarily narrow view, in my opinion– I think people should be free to make trust/cost/convenience tradeoffs. For example, a new user could avoid downloading the entire historical block chain by getting a snapshot of the ledger as of some recent block (“fetch the UTXO set”) from one or more nodes on the network. That is much faster and convenient than fetching the entire blockchain history, and is almost perfectly safe– even if you get a bad copy of the ledger, the worst that can happen is either there are extra entries in the ledger (maybe the attacker grants themselves a million bitcoin they don’t actually have), or there are entries missing from the ledger. If there are extra entries, then an attacker could send you an invalid transaction that would never confirm. They can do essentially the same thing today by just sending you a transaction and sending the rest of the network a double-spend of the transaction., If there are missing entries, your wallet might think the opposite– that a valid transaction is invalid. It will discover it’s mistake as soon as the transaction is verified in the block chain– and can recover by getting the ledger from a more reliable peer., Finally, there’s this O(n2) argument:, If we assume that there are ‘n’ users, and a constant proportion of them (say 1%) run full nodes, and each user generates a certain number of transaction per day… then the total validation work done by all the full nodes every day is O(n2).  (really, O((n/100)2), but constant factors are ignored in big-o notation), There are two things wrong with this argument; first, the assumption that a constant proportion of users will run full nodes as the network grows might be incorrect. It is likely a larger and larger proportion of people will choose to run partially-validating nodes. In the future there might be a billion people using bitcoin but only tens of thousands (instead of tens of millions) fully-validating nodes. That would be a bright, successful, decentralized, secure future., The second thing wrong with that argument is that while the entire network might, indeed, perform O(n2) validation work, each of the n individuals would only perform O(n) work– and that is the important metric, because each individual doesn’t care how much work the rest of the network is doing to validate transactions, they just care about how much work their computer must do.
I’m going to take a break from addressing objections to raising the maximum block size because in discussions I’m seeing people claim that “blocks aren’t full” or “we don’t yet have a functioning fee market.”, It is true that not every single block is one-megabyte big. But it is not entirely true that “blocks aren’t full” – many blocks are full, either because miners are deciding to create blocks smaller than one megabyte or because sometimes it takes a long time to find a block. Here is a graph I created (from Jameson Lopp’s excellent statoshi.info website) showing the memory pool– transactions waiting to be confirmed– over time: , Every peak on that graph corresponds to a block being found, and you can see that many blocks don’t clear out the memory pool., The biggest triangle on that graph, near the beginning, has a peak of about 4,500 transactions waiting to be confirmed. Near the end of the graph, you can see a couple of big triangles with spikes on the way up and/or the way down. Those are places where the memory pool started to back up and a miner who has decided (for whatever reason) to create smaller blocks found a block., In the middle, around time 00:00, you see lots of little wiggles– miners got lucky and found a bunch of blocks in one hour. It looks like the memory pool completely emptied out there (the line goes to zero)., If I had infinite time I’d submit a patch to statoshi to get a graph that excludes zero-fee transactions from the graph, but looking at the graph I’m certain that a lot of miners are filling smaller blocks and leaving some fee-paying transactions out., And since many miners are doing that, there is a functioning fee market, at least if you care about your transaction getting confirmed in the next one-or-two-or-three blocks:, , That’s a graph of Alex Morcos’ spiffy new fee estimation code (which I would be reviewing instead of writing this, if I had infinite time)., If you don’t care how long your transaction takes to be confirmed, then I agree– there is no functional fee market, the transaction spam limits kick in and put a floor on transaction fees., But we don’t need 100% full one megabyte blocks to start to learn about what is likely to happen as transaction volume rises and/or the one megabyte block size limit is raised.
Yesterday’s post triggered some interesting discussion on reddit and twitter., I realize now I should have been more specific– when I said “technical definition of Bitcoin” I meant a couple of things:, I am thinking of what Bitcoin is today and in the near future, not what it might eventually evolve into.  While it is fun to talk about what Bitcoin will be in 100 years, that wasn’t the point of my blog post. I didn’t mean to imply that any definition of what Bitcoin is today and in the near future would somehow be iron-clad and binding and never change; thinking you can control how a technology evolves is even sillier than thinking you’ll be able to predict beyond a decade or maybe two., I also want a definition that could be useful in determining which of two competing ledgers a neutral geek would point at and say “that one is Bitcoin as described in the original Bitcoin whitepaper”. It, Continue reading →, Engineers are great at not seeing the forest for the trees. They get stuck on details and lose track of the bigger picture., I’ve seen it most often (and have been guilty myself) when they’re optimizing something to make it faster. They’ll start out OK– “it is taking eleven seconds to agitate the snarks, and seven seconds of that is just precomputing the eigenwidgets!”, So they’ll take a day and make precomputing the eigenwidgets ten times faster., And then realize with just a little tweaking and a really nifty algorithm and two hundred more lines of code they can make it one hundred times faster!, So they spend a few days making snark agitation take 0.63 seconds faster (4.07 seconds instead of 4.7 seconds), instead of moving on to the next performance bottleneck. They can become focused on one little thing (Performance of this routine! or Security! or Decentralization! or, Continue reading →, Now that six months have gone past, I’m being asked if I still think Craig Wright was Satoshi., I think there are two possibilities., Either he was Satoshi, but really wants the world to think he isn’t, so he created an impossible-to-untangle web of truths, half-truths and lies. And ruined his reputation in the process., If he was Satoshi, we should respect his wish to remain anonymous, and ignore him., The other possibility is he is a master scammer/fraudster who managed to trick some pretty smart people over a period of several years., In which case everybody except the victims of his fraud and law enforcement working on behalf of those victims should ignore him., So, either he was or he wasn’t. In either case, we should ignore him. I regret ever getting involved in the “who was Satoshi” game, and am going to spend my time on more fun and productive pursuits., Continue reading →, A couple of months ago, I was having Yet Another Argument with a Bitcoin Core contributor about the one megabyte block size limit., I had asked: “what other successful Internet protocols impose arbitrary limits on themselves similar to the one-megabyte limit?”, His answer was “there are limits on the size of the global routing table in the Border Gateway Protocol (BGP) protocol.”, BGP is the protocol routers use to decide what to do when they get a data packet bound for an IP address like “111.12.1.1” – they send it to China (111.12.1.1 belongs to the China Mobile Communications Corporation right now)., So I went and skimmed through the BGP protocol documents. Then read the wikipedia page on BGP. And had the opportunity to talk with Justin Newton, who participated in the debates on Internet scalability fifteen years ago., And found that there are no protocol-level limits on the size of, Continue reading →, I believe Craig Steven Wright is the person who invented Bitcoin., I was flown to London to meet Dr. Wright a couple of weeks ago, after an initial email conversation convinced me that there was a very good chance he was the same person I’d communicated with in 2010 and early 2011. After spending time with him I am convinced beyond a reasonable doubt: Craig Wright is Satoshi., Part of that time was spent on a careful cryptographic verification of messages signed with keys that only Satoshi should possess. But even before I witnessed the keys signed and then verified on a clean computer that could not have been tampered with, I was reasonably certain I was sitting next to the Father of Bitcoin., During our meeting, I saw the brilliant, opinionated, focused, generous – and privacy-seeking – person that matches the Satoshi I worked with six years ago. And he cleared up a lot of mysteries, Continue reading →, I participated in the “Satoshi Roundtable” event in Florida last weekend on my way home from the Financial Cryptography conference., The Roundtable is an invitation-only event organized by Bruce Fenton (current Executive Director of the Bitcoin Foundation, but it is not a Foundation event), and I’m told that in past years it was mostly a relaxed networking/socializing event., This year, discussion of the the block size limit dominated the entire weekend., All of that discussion happened under the “Chatham House Rule” – “…participants are free to use the information received, but neither the identity nor the affiliation of the speaker(s), nor that of any other participant, may be revealed” – so for the rest of this blog post I will talk about what happened and what was said, but won’t mention any names or affiliations. You can see who attended at the event’s website., There were a couple, Continue reading →, A couple of months ago, Paul Sztorc published a blog post asking two very good questions:, For me, personally, the answers are simple. First, the limits were added to prevent a ‘poisonous block’ network denial-of-service attack. We have to worry about denial-of-service attacks if they are inexpensive to the attacker. ‘Amplification’ attacks are the worst, where the attacker sends a little bit of information that causes lots of traffic on the network or causes lots of wasted CPU processing., Second, a couple of key things have changed since Satoshi made the change., The attack the limit is meant to prevent is much more expensive today. I have a spreadsheet, Continue reading →, I don’t like arbitrarily chosen constants in code., Sometimes they’re unavoidable and harmless. I use 11 in code I write when the number doesn’t matter (because eleven is my favorite number)., I like arbitrarily chosen constants in protocols even less, but sometimes they’re unavoidable. The 2mb block limit proposal has a few ‘magic’ numbers, but the 75% hash-rate threshold and the 28-day grace period generate the most discussion and debate., Some people would rather the 75% be 95% or 99%. I think that is too high because it gives “veto power” to a single big solo miner or mining pool. Holding veto power is dangerous– somebody who disagrees with the change or just wants to disrupt Bitcoin might use extortion, bribery, or blackmail against a miner to try to prevent the change., Other people think 75% is too high; after all, if 51% of hash power got together they could just choose to ignore, Continue reading →, Increasing the block size limit from 1 million bytes to 2 million bytes sounds so simple:  just change the “1” to a “2” in the source code and we’re done, right?, If we didn’t care about a smooth upgrade, then it could be that simple. Just change this line of code (in src/consensus/consensus.h):, to:, If you make that one-byte change then recompile and run Bitcoin Core, it will work. You computer will download the blockchain and will interoperate with the other computers on the network with no issues., If your computer is assembling transactions into blocks (you are a solo miner or a mining pool operator), then things get more complicated. I’ll walk through that complexity in the rest of this blog post, hopefully giving you a flavor for the time and care put into making sure consensus-level changes are safe. , Github has a handy, Continue reading →, What would happen if some minority of mining hash power and maybe a merchant or exchange decided to stay with, or move to, different consensus rules than everybody else?, Would there be two different flavors of Bitcoin? Would it cause massive disruptions to the Bitcoin economy?  Would your coins be safe?, (spoiler alert if you’re in a hurry: no, no, and yes), I’ll start with the assumption that there is a supermajority (two-thirds or more– comfortably over 50%) that wants one set of consensus rules, and a minority that wants another set of consensus rules. This analysis doesn’t work if there is an even split in opinion about the rules. I’m also assuming that there is a supermajority of both hash power and transaction creators (the ‘economic majority’) on the same side; the analysis is different if miners and exchanges/merchants/users disagree about what the rules should be., So, if there
Almost two years ago, when I stepped down as lead maintainer for Bitcoin Core, I wrote:, I’d still like to focus on protocol-level, cross-implementation issues but lately I’ve been distracted and have generated a lot of controversy (and hurt feelings) by helping out with some other implementations (first XT, lately Bitcoin Classic, maybe Bitcoin Unlimited soon., Madness! Chaos! ANARCHY! … I hear some people say, but there is a method to my madness. When I was lead maintainer of Core I had the following top-three priorities:, 1) Keep the system secure.2) Keep the network reliably processing transactions.3) Eliminate single points of failure., Those are still my top priorities, but I try to take a higher-level view, looking at the entire Bitcoin ecosystem instead of just the Core software implementation. So if those are my top priorities, what should I be working on?, Bitcoin-the-protocol is doing really well security-wise; the responsibly-report-a-vulnerability-in-bitcoin mailing list has been quiet for many months. And seeing multi-signature “p2sh” adopted is very personally satisfying. I’m still active asking dumb questions to people smarter than me to try to avoid mistakes as the protocol evolves., I’m still worried about reliability of the network in the short term, which is why I’ve been so vocal on the block size limit issue, and which is part of the reason I’m supporting alternatives to Bitcoin Core. In the long run I think everything will work out fine, no matter what happens with the block limit., Clever engineers will find ways to work around around the limit, whether that is ‘extension blocks’ or the lightning network or a sidechain that everybody moves their coins to doesn’t really matter. I’d prefer a nice, simple, clean solution, but I’m old enough to know that most of the world’s great technologies are built on top of horrifying piles of legacy cruft, and they work just fine pretty much all of the time., And Bitcoin will survive without a short-term solution, but adoption and growth might be set back a year or two., What to do about the short-term scaling issues brings me to my third priority: eliminating single points of failure. Also known as “decentralize all the things”/“diversity is good”/“competition is good”., Part of eliminating single points of failure is trying different solutions to a problem at the same time. Assuming you have the resources, that is a good strategy– you increase your chances of success, because you only fail to solve the problem if all of the solutions fail., Apply that reasoning to development teams and, assuming you have the resources, it is better to have multiple teams because there will be less disruption if one of them fails., Six years ago the Bitcoin ecosystem didn’t have the resources to support multiple development teams – it certainly does now., Specialization is natural as technologies mature; Ford Motor Company used to have raw materials delivered to one end of its enormous factory and turn out finished cars at the other end. Today, Ford assemble parts made from thousands of suppliers, and try to make sure they have more than one supplier for everything to eliminate single points of failure., I’m doing what I can to speed up this natural process for Bitcoin. I’ll be contributing my advice and experience (and code review and code) to Bitcoin Classic and Bitcoin Unlimited and Bitcoin Core and maybe other open source projects that come along. I hope that they are all successful, and figure out where in the Bitcoin ecosystem they should specialize– maybe Bitcoin Classic will be the preferred distribution for miners and Bitcoin Unlimited will focus on features for end-users. Maybe Bitcoin Core will decide to focus on longer-term improvements at the core protocol level., Or perhaps they will specialize by having different development cultures or processes for making decisions. If that sounds crazy to you, I’d encourage you dig into how various Linux distributions differ in priorities, decision-making, culture, etc. Diversity is good., But what if the various implementations can’t agree on something they all need to do the same way? Linux has Linus – who gets to make the final decision on what the Bitcoin consensus rules are?, Satoshi answered that question in the last sentences of his whitepaper:, I plan on writing more about incentives and the process of coming to consensus in the near future.
I’m adding entries to my list of objections to raising the maximum block size as I run across them in discussions on reddit, in IRC, etc., This is the technical objection that I’m most worried about:, More transactions means more memory for the UTXO database, I wasn’t worried about it yesterday, because I hadn’t looked at the trends. I am worried about it today., But let me back up and explain briefly what the UTXO database is. UTXO is geek-speak for “unspent transaction output.” Unspent transaction outputs are important because fully validating nodes use them to figure out whether or not transactions are valid– all inputs to a transaction must be in the UTXO database for it to be valid. If an input is not in the UTXO database, then either the transaction is trying to double-spend some bitcoins that were already spent or the transaction is trying to spend bitcoins that don’t exist., Jameson Lopp’s statoshi.info web site keeps track of the size of the UTXO database over time. Over the last 11 months it looks like this:, So the UTXO size has doubled in the last year. That would be OK if the price of memory was halving every year, but memory prices dropped only about 20% last year. Even assuming Moore’s Law kicks in again as new memory chip technologies roll out we would still have the UTXO set growth outpacing the advance of technology., Today, the UTXO database is about 650MiB on disk, 4GB when decompressed into memory. DRAM costs about $10 per GB, so you need to spend about $40 on memory if you want absolute fastest access to the UTXO. Not a big deal., Assuming the UTXO set continues to double and RAM prices continue to drop 20% per year, next year you’ll have to spend about $64. Ten years from now, over $4,000…, … except the maximum block size will stop the exponential growth. A one megabyte block is room for about 100 million 500-byte transactions per year. If every one of them increased the UTXO set by 500 bytes, that would grow the UTXO set 50 gigabytes a year. So very worst case running a full node with the entire UTXO set in RAM is $500 per year, at today’s DRAM prices. That cost would decline as memory prices fell., Allowing more transactions with no other changes would very likely accelerate the UTXO set growth, making it more expensive, more quickly, to run a fully validating node. And worst case would be twenty times as expensive; $10,000 per year. Affordable for a business, not for an individual., That is a very good reason to oppose increasing the maximum block size., But is the worst case realistic? The entire UTXO set doesn’t have to be in RAM; it can be stored on an SSD or spinning hard disk. The access pattern to the UTXO is not random; outputs that were spent recently are more likely to be re-spent than outputs that have not been spent in a long time. Bitcoin Core already has a multi-level UTXO cache, thanks to the hard work of Pieter Wuille., Solid-state-disk (SSD) prices are about $0.50 per GB, spinning disks are about $0.05 per GB. Worst case UTXO storage for 20MB blocks is about one terabyte per year, or $500 per year at today’s SSD prices. $50 per year at today’s hard disk prices., That’s not so bad! , But there is a tradeoff – if you don’t store the entire UTXO set in DRAM, then it will take you longer to validate blocks. How much longer depends on how much memory you’re dedicating to the UTXO cache, the speed of your SSD or hard drive, and how well a new block’s transactions match typical transaction patterns., Unless you are mining, taking longer to validate new blocks isn’t a problem as long as you can validate them fast enough to keep up with the 10-minute-average block time., If you are mining, then taking longer to validate new blocks puts you at a disadvantage, because you can’t (or, at least, shouldn’t) start mining on the new best chain until you’ve fully validated the new block. I’ll write about that more when I respond to the “Bigger blocks give bigger miners an economic advantage” objection.
Yesterday’s post triggered some interesting discussion on reddit and twitter., I realize now I should have been more specific– when I said “technical definition of Bitcoin” I meant a couple of things:, I am thinking of what Bitcoin is today and in the near future, not what it might eventually evolve into.  While it is fun to talk about what Bitcoin will be in 100 years, that wasn’t the point of my blog post. I didn’t mean to imply that any definition of what Bitcoin is today and in the near future would somehow be iron-clad and binding and never change; thinking you can control how a technology evolves is even sillier than thinking you’ll be able to predict beyond a decade or maybe two., I also want a definition that could be useful in determining which of two competing ledgers a neutral geek would point at and say “that one is Bitcoin as described in the original Bitcoin whitepaper”. It is very possible I’m not seeing the forest for the trees, and I should think of the whole social infrastructure that is Bitcoin and accept the fact that “Bitcoin” is a human-generated concept that cannot be pinned down. But I think that’s stupid, and even imperfect definitions can help to bring clarity., I found two suggestions for modifying the definition interesting. First, instead of talking about double-SHA256-proof-of-work, just say “chain with most energy expended on proof of work.”  I do think that if SHA256 was ever horribly broken the proof-of-work could change and the result should still be called “Bitcoin”., That other is to declare that “Bitcoin” is the ledger starting with the genesis block that has the biggest market cap (exchange rate times number of coins) instead of most proof-of-work. That probably matches people’s intuitive notion better than proof-of-work, and if there was some fast, secure way to determine which branch of the ledger has the larger market cap it would even be a useful technical/engineering definition., In practice, the chain with the biggest market cap will be the chain with the most proof-of-work. I can imagine extremely unlikely scenarios involving economically irrational miners trying to destroy Bitcoin where that isn’t true, but Bitcoin’s two-week difficulty adjustment period makes it expensive for a minority of hashpower to maintain a split in the blockchain.
I just listened to Emin Gün Sirer and Ittay Eyal from Cornell University on the Epicenter Bitcoin podcast., They’re doing great work; full-scale emulation of the Bitcoin network is a fantastic idea, and I plan on doing a lot of testing and optimizations using the tools they’ve developed. I also plan on writing about their Bitcoin NG idea… but not right now., Listening to the podcast, and listening to complaints about Bitcoin XT from one of the other Core committers, I realized there’s a fundamental disagreement about protocol design., The most successful protocols were forward-looking. When the IP protocol was designed in 1970’s, the idea of 4 billion computers connected to a single network was ludicrous. But the designers were forward-looking and used 32-bits for IP addresses, and the protocol grew from a little research project to the global internet that is just now, 40 years later, running out of IP addresses., I applaud Gün and Ittay for getting scientific about the Bitcoin network, and establishing metrics that can be used to evaluate implementations or proposals. But I think it is too easy to get anchored to the Bitcoin network as it is implemented today, and I don’t think the network as it is implemented right now in the Bitcoin Core reference implementation should dictate high-level protocol design., I think protocol design should be forward-looking, and protocol design should not be tied to one particular implementation., I understand the desire to be conservative, and to test at the limits of whatever the protocol allows. One of the criticisms of the BIP101 proposal I’ve heard from some people is “you haven’t tested the network with gigabyte blocks.” I wonder if the IP designers had colleagues who complained “we haven’t tested the network with a billion computers” – and I wonder what protocol we’d be using on the Internet today if the designers of the IP protocol hadn’t been so forward-looking., I keep hearing that bigger blocks might drive mining centralization, but I wrote about that earlier this year and still haven’t seen a convincing simulation or argument that is true, unless you assume that the current p2p protocol is set in stone and will never be changed., I’m going to work on a better protocol for broadcasting transactions and blocks across the network, because if we want miners to be willing to create much bigger blocks, a better protocol is needed (we already have one in the form of Matt Corallo’s “fast relay network”, which is a big reason most mining pools are willing to create one-megabyte blocks). But I think it would be a mistake to wait until that work is done to schedule the protocol change to allow bigger blocks, for three reasons:, First, because it takes about six months for any protocol change to get deployed across the network., Second, because somebody else might have an even better idea than me. With a one megabyte block size limit, there is little incentive to work on optimizing transaction or block propagation– why spend a lot of time writing code that will only be relevant if the maximum block size is raised?, And finally, because miners aren’t stupid. When slush produced a 900+ kilobyte block that forked the chain, the biggest miners immediately agreed to produce smaller blocks until that Bitcoin Core software was fixed.
I think I first heard this objection to a larger maximum block size from Peter Todd, Chief Scientist of ViaCoin:, More transactions makes it more difficult to keep Bitcoin activity private from oppressive governments., That is hard to argue with– obviously the more you do something, the more likely you will get noticed. But engineering is about tradeoffs, so how much more likely is somebody to get noticed by their oppressive government if they are using a Bitcoin system that supports sixty, rather than three, transactions per second?, Well, if they are just transacting with Bitcoin using a lightweight wallet, there is no difference. Lightweight wallets (meaning any wallet except for Bitcoin Core or Armory) only download transactions relevant to them. They are unaffected by changes to the maximum block size., But what if you want to be a miner in some oppressive country?, Well, if you have good connectivity to a mining pool outside the country you should be able to connect to it over Tor. And, again, you will be unaffected by changes to the maximum block size, because you just need 80-byte block headers., But what if you are in an oppressive country and you want to participate in the network as a full node, or want to solo mine, or want to use “getblocktemplate” when mining so you have complete control over what transactions you mine?, Well, in that case the simplest solution is to pay $10 per month to a hosting provider in a country that is NOT oppressive. Run a full node there, and if you have mining equipment in your house, connect to it over an encrypted connection (an ssh tunnel would work nicely). Again, bandwidth from your oppressive internet service provider will be the same no matter how busy the Bitcoin network gets., But what if you don’t have $10 per month to spend?, My advice would be to stick with running a lightweight wallet running over Tor, or connecting to a mining pool via Tor., You could run a full node over Tor, but even with one megabyte blocks that would be over 100 megabytes of encrypted Tor traffic every day. The risk of jack-booted thugs breaking down your door and demanding to know what you are doing far outweigh the benefits of running a fully validating node.
A couple of months ago, Paul Sztorc published a blog post asking two very good questions:, For me, personally, the answers are simple. First, the limits were added to prevent a ‘poisonous block’ network denial-of-service attack. We have to worry about denial-of-service attacks if they are inexpensive to the attacker. ‘Amplification’ attacks are the worst, where the attacker sends a little bit of information that causes lots of traffic on the network or causes lots of wasted CPU processing., Second, a couple of key things have changed since Satoshi made the change., The attack the limit is meant to prevent is much more expensive today. I have a spreadsheet with all of the trades made on bitcoinmarket.com, the earliest Bitcoin exchange. On July 15th, about eleven thousand bitcoin were traded at an average price of about three cents each., The block reward was 50 BTC back then, so miners could sell a block’s worth of coin for about $1.50.  That gives a rough idea of how much it would cost an attacker to produce a ‘poisonous block’ to disrupt the network– a dollar or two. Lots of people are willing to spend a dollar or two “for the lulz” – they enjoy causing trouble, and are willing to spend either lots of time or a modest amount of money to cause trouble., Today the block reward is 25 BTC and the price is over $400; miners get over $10,000 for the blocks they produce. An attacker would have to spend close to that amount to produce a ‘poisonous block.’, Something else has changed since July of 2010– we know that attackers could produce very-expensive-to-validate blocks, even with the limits Satoshi imposed over five years ago.  We have known that since Sergio Demian Lerner reported it in 2013., The fact that nobody has attacked the network by producing intentionally-expensive-to-validate blocks is very good evidence that there is no profit to be made by the attack… but it is a good idea to completely eliminate the possibility of a ‘poisonous block’ attack. BIP 109 eliminates the attack in a straightforward way, and makes it safe to raise the limit., So, we know how to safely raise the limit. In 2010 there were a few hundred transactions confirmed per day. Today there are hundreds of thousands confirmed every day. The number of people reporting trouble getting their transactions confirmed is increasing, and even the smartest fee-estimating code in the world will not prevent transaction confirmations from becoming increasingly unreliable., See Paul’s blog post for other people’s answer to “why do we have a maximum block size.”, In my view, people are using the block size limit for something it was never meant to do– to influence how people use the Bitcoin blockchain, forcing some uses off the blockchain., They are doing this for the best of reasons– they believe their vision for the future of Bitcoin is better than what will naturally arise if the limits are (safely) raised or eliminated., I cannot support that type of top-down, centrally-planned vision.
A couple of months ago, I was having Yet Another Argument with a Bitcoin Core contributor about the one megabyte block size limit., I had asked: “what other successful Internet protocols impose arbitrary limits on themselves similar to the one-megabyte limit?”, His answer was “there are limits on the size of the global routing table in the Border Gateway Protocol (BGP) protocol.”, BGP is the protocol routers use to decide what to do when they get a data packet bound for an IP address like “111.12.1.1” – they send it to China (111.12.1.1 belongs to the China Mobile Communications Corporation right now)., So I went and skimmed through the BGP protocol documents. Then read the wikipedia page on BGP. And had the opportunity to talk with Justin Newton, who participated in the debates on Internet scalability fifteen years ago., And found that there are no protocol-level limits on the size of BGP routing tables. There is no place in any BGP specification I can find that says “Routing Tables Shall Be No More Than Eleven Gigabytes Big.”, BGP is interesting, because it is a completely different way of doing decentralized consensus than Bitcoin. Instead of proof-of-work, BGP’s consensus is built on real-world trust relationships between people operating the networks that make up the Internet. That works surprisingly well most of the time, especially considering the stunning lack of security in the BGP protocol., There are limits on routing table sizes, but they are not top-down-specified-in-a-standards-document protocol limits. They are organic limits that arise from whatever hardware is available and from the (sometimes very contentious!) interaction of the engineers keeping the Internet backbone up and running., I haven’t been able to find a widely-used Internet-scale protocol that arbitrarily limits itself. The closest I’ve found is the Simple Mail Transport Protocol (SMTP) which has a SIZE header that a server can use to specify the maximum email message size it will accept. The arbitrary limit chosen by the SMTP designers is 99,999,999,999,999,999,999 bytes (just under 100 exabytes)., The HTTP 2.0 spec explicitly discusses denial-of-service attacks, but doesn’t impose hard limits: “An endpoint MAY treat activity that is suspicious as a connection error of type ENHANCE_YOUR_CALM” (I can’t help imagining the server as a California Surfer saying “enhance your calm, dude!), That’s a well-designed spec. Trust that smart developers will fix scaling or denial-of-service issues as they arise– and, if for some unforeseen reason it turns out they can’t, trust that there will be either an amendment to the spec or a "best practices” document to fix the problem(s).
I’ve been accused of being too flippant about increasing the block size limit. This series of blog posts is meant to show that I’m not, that I have carefully thought about risks and benefits. I stepped back from the role of lead committer exactly so I would have the time to think about bigger-picture issues like this one. Today I’d like to address, head-on, this argument against changing the one-megabyte blocksize limit:, Larger-than-one-megabyte blocks have had insufficient testing and/or insufficient research into economic implications and/or insufficient security review of the risks versus benefits., This is tough to respond to– there can always be more testing or research, especially for a security-critical project like Bitcoin. It is easy to suffer from “analysis paralysis,” and I think the Core Bitcoin project has been suffering from analysis paralysis over the block size issue for at least three years now., I’m convinced the uncertainty over when or if this will be resolved is harming Bitcoin. If somebody can point me at a successful software technology that went through years and years of debate and research and was not deployed until it was perfect I’d change my mind – the example that immediately comes to my  mind is Project Xanadu versus the Internet., I don’t think we should adopt the Silicon Valley mantra of “move fast and break things.” But I do think we need to move– “stay still and watch things break” is just as bad.
I don’t like arbitrarily chosen constants in code., Sometimes they’re unavoidable and harmless. I use 11 in code I write when the number doesn’t matter (because eleven is my favorite number)., I like arbitrarily chosen constants in protocols even less, but sometimes they’re unavoidable. The 2mb block limit proposal has a few ‘magic’ numbers, but the 75% hash-rate threshold and the 28-day grace period generate the most discussion and debate., Some people would rather the 75% be 95% or 99%. I think that is too high because it gives “veto power” to a single big solo miner or mining pool. Holding veto power is dangerous– somebody who disagrees with the change or just wants to disrupt Bitcoin might use extortion, bribery, or blackmail against a miner to try to prevent the change., Other people think 75% is too high; after all, if 51% of hash power got together they could just choose to ignore blocks that vote ‘the wrong way’. If they are mining pools they might lose most of their miners, but they could., Miners producing up-version blocks is a coordination mechanism. Other coordination mechanisms are possible– there could be a centrally determined “flag day” or “flag block” when everybody (or almost everybody) agrees that a change will happen. , Some people think a 28 day grace period is not long enough for businesses or people to upgrade, but I’ve contacted several of the major Bitcoin miners, exchanges, web-wallet providers and they are all confident four weeks is “plenty of time” for them to upgrade., A longer grace period would be appropriate if mobile wallets needed to upgrade, because getting changes to apps approved and into the Apple or Android stores can take a while. But none of the popular mobile wallets are affected by a change to the block size limit., We can look at the adoption of the last major Bitcoin core release to guess how long it might take people to  upgrade.  0.11.0 was released on 12 July, 2015.  Twenty eight days later, about 38% of full nodes were running that release.  Three months later, about 50% of the network was running that release, and six months later about 66% of the network was running some flavor of 0.11., I expect adoption of the 2mb change will be faster, because once 50% of hashpower supports the change Bitcoin Core will complain “This version is obsolete; upgrade required!”., 28 days is plenty of time for hashpower to upgrade. At the last block version upgrade (for BIP 65), it took about a month to get to 75% of hashpower, then less than seven more days to reach 95%. By the end of the grace period, I would expect over 99% of hashpower to be ready for the first bigger-than-one-megabyte block.
Continuing with objections to raising the maximum block size:, More transactions means more bandwidth and CPU and storage cost, and more cost means increased centralization because fewer people will be able to afford that cost., I can’t argue with the first part of that statement– more transactions will mean more validation cost. But how much? Is it significant enough to worry about?, I’ll use ChunkHost’s current pricing to do some back-of-the-envelope calculations. I’m not affiliated with ChunkHost– I’m using them for this example because they accept Bitcoin and I’ve been a happy customer of theirs for a while now (I spun up some ‘chunks’ to run some 20 megabyte block tests earlier this year)., CPU and storage are cheap these days; one moderately fast CPU can easily keep up with 20 megabytes worth of transactions every ten minutes., Twenty megabytes downloaded plus twenty megabytes uploaded every ten minutes is about 170 gigabytes bandwidth usage per month – well within the 4 terabytes/month limit of even the least expensive ChunkHost plan., Disk space shouldn’t be an issue very soon– now that blockchain pruning has been implemented, you don’t have to dedicate 30+ gigabytes to store the entire blockchain., So it looks to me like the actual out-of-pocket cost of running a full node in a datacenter won’t change with a 20 megabyte maximum block size; it will be on the order of $5 or $10 per month., I chose 20MB as a reasonable block size to target because 170 gigabytes per month comfortably fits into the typical 250-300 gigabytes per month data cap– so you can run a full node from home on a “pretty good” broadband plan. , So if running a full node costs so little why are we seeing the number of full nodes on the network declining, even with one megabyte blocks? Won’t bigger blocks accelerate that decline?, And why are companies outsourcing the running of nodes and using API services like those offered by BitPay or Coinbase or Chain? Again, won’t bigger blocks accelerate that trend?, I think the answer to both of those questions is “no” – or, at least, “not significantly.” I agree with Jameson Lopp’s conclusion on the cause of the decline in full nodes– that it is “a direct result of the rise of web based wallets and SPV (Simplified Payment Verification) wallet clients, which are easier to use than heavyweight wallets that must maintain a local copy of the blockchain.” Give the typical user three experiences: a SPV wallet, a full node processing one megabyte blocks, and a full node processing twenty megabyte blocks, and they will choose SPV every time., Hosting and bandwidth costs of $10 per month are trivial even to a cash-starved startup. Finding and hiring good people is expensive, and that is what is driving companies to outsource blockchain maintenance. A larger maximum block size won’t change that equation.
There are two related arguments I hear against raising the maximum block size:, There is no need to raise the maximum block size because the Lightning Network / Sidechains / Impulse / Factom solves the scaling problem., If the maximum block size is raised, there will be little incentive for other scaling solutions to emerge, The first is easy to address: if any of the other proposed scaling solutions were tested, practical and already rolling out onto the network and being supported by all the various Bitcoin wallets then, indeed, there would be no hurry to schedule a maximum block size increase., Unfortunately, they’re not; read Mike Hearn’s blog post for details. We are years away from a time when we can confidently tell a wallet developer “use this solution to give your users very-high-volume, very-low-cost, very-low-minimum-payment instant transactions.”, The second is also easy to address. The “layer 2” services that are being built on top of the blockchain are absolutely necessary to get nearly instant real-time payments, micropayments and high volume machine-to-machine payments, to pick just three examples. The ten-minute settlement time of blocks on the network is not fast enough for those problems, and it will be the ten minute block interval that drives development of those off-chain innovations more than the total number of transactions supported., Scheduling an increase to the maximum block size now is a short-term, “kick the can down the road” fix. It is ugly, but necessary.
I participated in the “Satoshi Roundtable” event in Florida last weekend on my way home from the Financial Cryptography conference., The Roundtable is an invitation-only event organized by Bruce Fenton (current Executive Director of the Bitcoin Foundation, but it is not a Foundation event), and I’m told that in past years it was mostly a relaxed networking/socializing event., This year, discussion of the the block size limit dominated the entire weekend., All of that discussion happened under the “Chatham House Rule” – “…participants are free to use the information received, but neither the identity nor the affiliation of the speaker(s), nor that of any other participant, may be revealed” – so for the rest of this blog post I will talk about what happened and what was said, but won’t mention any names or affiliations. You can see who attended at the event’s website., There were a couple of key moments that I remember. At one point, everybody was asked if they supported the “Hong Kong compromise” from the week before (segregated witness in April, then code for a 2MB hard fork in July of 2016 with a minimum of a year before 2MB blocks are allowed)., “Everybody who support that, raise your hand” :  a dozen or so people, most of whom were part of that Hong Kong meeting, raise their hands., “Everybody who does not support that, raise your hand” : everybody else (forty? fifty people?) raises their hands., There was a lot of talk about moving the July of 2017 date closer. In particular, a plan was presented that had a three month grace period after 95% of hashpower voted yes and 75% of coin-age-weighted-transaction-volume-in-some-time-period also indicated support., A subset of people spent several hours talking about the details, but reached no consensus. 95% miner voting is a problem for some miners, who don’t want to have veto power over such an important change. The chances of extortion either against them (“vote the way I want or I will DDoS you off the Internet”) or by them (“If you want me to vote a certain way, pay up”) are just too great., And while the 75% coin-age voting is an interesting idea, actually agreeing on the details (Is there a fixed time period when the vote happens? A sliding window with a deadline? Should it be 75% of total transaction volume or just 75% of the volume that bothers to express a preference? Should it be 25% can veto instead of 75% needed to approve? Is this meant to be a vote or just a signal that people have upgraded?)  and then writing and reviewing the code would take months., Over the last year of trying, and failing, to reach a reasonable compromise, it has become clear to me that some developers don’t want any on-chain scaling solution any time soon. They believe more theoretically elegant (but technically complicated) off-chain solutions like the Lightning Network are a better long-term scaling solution, and they believe that by resisting a simple limit increase we will get to that long-term solution faster., They are wrong., If the block size limit is not increased, we will see more off-chain solutions. But they won’t be the Lightning Network– instead, we will see highly centralized “clearing” agreements between exchanges and miners and merchants or merging of miners and transactions creators., We can see this start to happen– BTCC is a large exchange and mining pool, and announced “BlockPriority” last year for its customers. I don’t blame BTCC at all for doing that, it makes perfect business sense., It is, however, a symptom of an unhealthy network that is becoming increasingly unreliable and vulnerable to attacks., Brian Armstrong and Bruce Fenton also wrote about this year’s Satoshi Roundtable.
I was planning to submit a pull request to the 0.11 release of Bitcoin Core that will allow miners to create blocks bigger than one megabyte, starting a little less than a year from now. But this process of peer review turned up a technical issue that needs to get addressed, and I don’t think it can be fixed in time for the first 0.11 release., I will be writing a series of blog posts, each addressing one argument against raising the maximum block size, or against scheduling a raise right now. These are the objections I plan on writing about; please send me an email (gavinandresen@gmail.com) if I am missing any arguments for why one megabyte is the best size for Bitcoin blocks over the next few years., If I am being unfair in how I am stating any of the above objections, please let me know via email.
I believe Craig Steven Wright is the person who invented Bitcoin., I was flown to London to meet Dr. Wright a couple of weeks ago, after an initial email conversation convinced me that there was a very good chance he was the same person I’d communicated with in 2010 and early 2011. After spending time with him I am convinced beyond a reasonable doubt: Craig Wright is Satoshi., Part of that time was spent on a careful cryptographic verification of messages signed with keys that only Satoshi should possess. But even before I witnessed the keys signed and then verified on a clean computer that could not have been tampered with, I was reasonably certain I was sitting next to the Father of Bitcoin., During our meeting, I saw the brilliant, opinionated, focused, generous – and privacy-seeking – person that matches the Satoshi I worked with six years ago. And he cleared up a lot of mysteries, including why he disappeared when he did and what he’s been busy with since 2011. But I’m going to respect Dr. Wright’s privacy, and let him decide how much of that story he shares with the world., We love to create heroes – but also seem to love hating them if they don’t live up to some unattainable ideal. It would be better if Satoshi Nakamoto was the codename for an NSA project, or an artificial intelligence sent from the future to advance our primitive money. He is not, he is an imperfect human being just like the rest of us. I hope he manages to mostly ignore the storm that his announcement will create, and keep doing what he loves– learning and research and innovating., I am very happy to be able to say I shook his hand and thanked him for giving Bitcoin to the world.
Discussions of raising the maximum block size often turn into discussions of Bitcoin’s very-long-term future – “but what about when the block reward goes to zero” or “but how, exactly, will it all work if Bitcoin is used by billions of people per day…”, I’m going to upset a lot of engineers, but I think it is a mistake to try to predict what is going to happen that far in the future, and an even bigger mistake to spend a lot of time now worrying about what might happen ten or twenty years from now., I’m inspired by F.A. Hayek’s “The Fatal Conceit”:, The curious task of economics is to demonstrate to men how little they really know about what they imagine they can design., To the naive mind that can conceive of order only as the product of deliberate arrangement, it may seem absurd that in complex conditions order, and adaptation to the unknown, can be achieved more effectively by decentralising decisions, and that a division of authority will actually extend the possibility of overall order., ‘Aha!’ you say: ‘Hypocrite! You’re proposing 20MB blocks, and that’s the very definition of centralized decisionmaking!’, Yes, that’s true– it is an attempt at a short-term compromise, to address the fears that, if left unchecked, the block size might grow too large for anybody but large companies or wealthy individuals to fully validate the entire blockchain., I’ll discuss alternatives to that compromise in my next blog post.
I’m still seeing variations on this argument for keeping the one-megabyte block limit:, The network will be more secure with one megabyte blocks, because there will be more competition among transactions, and, therefore, higher transaction fees for miners., I’ve written before about the economics of the block size (and even went to the trouble of having Real™ Economists review it), but I’ll come at that argument from a different angle., Miner rewards expressed in BTC are:, block reward + ( number of transactions x average fee ), In the currencies that most miners care about right now (dollars or euros or yuan) rewards are:, (reward + number transactions x average_fee) x exchange rate, The fear is that as the block reward diminishes, either the number of transactions or the average transaction fee will not rise enough to replace the block reward. So miners will drop out, and the network will be less secure against an attacker with a lot of mining power (or, equivalently, it would cost an attacker less to amass enough mining power to successfully attack the network)., Focusing on just the average_fee term in that equation is a mistake; the number of transactions is just as important. If the goal is to maximize the #transaction x average_fee part of the equation, we would need to know the “price elasticity of demand” for bitcoin transactions to calculate the most-profitable-to-miners maximum block size., But the exchange rate is even more important, at least until the reward drops to zero. So if the goal is to keep the network as secure as possible over the next ten or fifteen years, we should be focusing on doing things that are likely to increase the exchange rate. All of the things that come to my mind that make Bitcoin more valuable (like increasing the number of people using Bitcoin, increasing security using multisig wallets, allowing people to innovate and leverage the security of the blockchain for things other than payments) will benefit from a larger maximum block size.
After taking a break to help review some pull requests and take a trip to New York, I’m ready to continue tackling objections to increasing the block size:, Bigger blocks give bigger miners an economic advantage, This is a hard blog post to write; the arguments for why bigger blocks give bigger miners an economic advantage over smaller miners are highly technical (whenever I use the term “miner” in this post I really mean “solo miner or mining pool”)., To start: think about what happens when a miner gets lucky and finds a new block. They send it to their peers and immediately start working on finding another block on top of that new block., Their peers will receive and validate the block, and, assuming validation passes, they then relay the block to their peers and start mining on top of the new block. The original miner is busy working during this whole validate-then-relay process; they have a head-start because they know about the new block before anybody else., If all the miners have about the same hashing power, then the head-start doesn’t matter– they all benefit about the same amount. , If the miners don’t all have the same hashing power, then it gets complicated. Bigger miners have an advantage, but how much of an advantage?, I ran some simulations, and if blocks take 20 seconds to propagate, a network with a miner that has 30% of the hashing power will get 30.3% of the blocks., To put that extra 0.3% in perspective, here are a couple of recent profitability calculation froms the Neighbourhood Pool Watch blog:, 0.3% is in the noise, miner profitability varies much more than that from week to week., We’ve just started to optimize block propagation for Bitcoin Core (see pull request #6077 or Matt Corallo’s high-speed relay network for example), and I’m confident that we will have 20MB blocks propagating across the network more quickly than 1MB blocks propagate today, eliminating even that small 0.3% advantage., Longer term, I’m also confident smarter synchronization algorithms will get even much larger blocks propagating even more quickly., All of the above assumes that miners are economically rational; that they’re not trying to mine blocks that destroy the network., What if that is a bad assumption? Do 20MB blocks increase the risk that somebody with a lot of money to burn will try to use it to take down the Bitcoin network? Are there attacks possible with 20MB blocks that are not possible with 1MB blocks?  Lets look at some possible attacks:, If a miner can create blocks that takes other miners a very long time to validate, and the miner is trying to attack the network and has a lot of hashing power, it becomes easier for that miner to dominate the network to censor or roll-back transactions., I find it hard to worry much about this attack, because I find it difficult to believe that an economically irrational attacker would have the resources to successfully mount an attack with expensive-to-validate blocks, but would not have the resources to simply mount a brute-force 51% or selfish-mining attack. I find all of the “invest a lot of money or effort to subvert mining” attacks implausible; mounting a sustained DDOS attacks against the network would probably be much less expensive and more effective., Nevertheless, I will propose that 20MB blocks limit transactions to 100kilobytes to mitigate this theoretical attack (which we’ve known about for over two years now;  see this discussion)., Maybe you’re not worried about marginal, malicious miners, but you are worried that an honest miner might be tricked into creating a block that propagates slowly., For example, an attacker might create 20 megabytes worth of double-spent transactions, and give the network one set of spends but feed a target miner the other set of spends. That will make the miner spend more time validating blocks that are found by the network, and will make the miner’s blocks propagate more slowly, because normally a block is full of transactions that the network has already seen and validated., I’m not worried about this attack, because I’ve already benchmarked how long it takes to validate a 20MB block full of never-before-seen transactions– it takes about twenty seconds, which is simply not a big deal with a 600-second block time. And Bitcoin Core already has rules in place to prevent attackers from giving miners transactions that take a very long time to validate., If the attacker’s goal is to put the honest miner out of business, this is a poor attack; it will decrease the number of blocks found by the honest miner by a tiny fraction of a percent., If the attacker’s goal is to pull off a successful 0-confirmation double-spend, this is also a very poor attack. The transaction fees required to stuff a 20MB block full of the attacker’s transactions will easily overwhelm any marginal advantage they might gain from causing a slower-to-propagate-than-normal block., I covered the cost of running a full node in a previous post. Even without any further optimizations (and we are busy optimizing) it is not expensive to run a node capable of validating 20MB blocks., I was worried that I would find out 20MB blocks would give big miners a significant advantage before I started working on this post a week ago. I’m not worried any more, but please send me email if there is something I’ve missed.
Engineers are great at not seeing the forest for the trees. They get stuck on details and lose track of the bigger picture., I’ve seen it most often (and have been guilty myself) when they’re optimizing something to make it faster. They’ll start out OK– “it is taking eleven seconds to agitate the snarks, and seven seconds of that is just precomputing the eigenwidgets!”, So they’ll take a day and make precomputing the eigenwidgets ten times faster., And then realize with just a little tweaking and a really nifty algorithm and two hundred more lines of code they can make it one hundred times faster!, So they spend a few days making snark agitation take 0.63 seconds faster (4.07 seconds instead of 4.7 seconds), instead of moving on to the next performance bottleneck. They can become focused on one little thing (Performance of this routine! or Security! or Decentralization! or Compatibility!) and ignore everything else., I’d like to propose this big-picture technical definition of “Bitcoin”:, “Bitcoin” is the ledger of not-previously-spent, validly signed transactions contained in the chain of blocks that begins with the genesis block (hash 000000000019d6689c085ae165831e934ff763ae46a2a6c172b3f1b60a8ce26f), follows the 21-million coin creation schedule, and has the most cumulative double-SHA256-proof-of-work.1, If we can agree that is what we mean when we say “Bitcoin” then I think a lot of needless argument about “the trees” might be avoided., Is there a nifty new type of transaction that is accepted by majority hashrate?  Yes, still Bitcoin. Different arrangement of the merkle tree in the block header? Yes, still Bitcoin. Fix the off-by-one error in the difficulty retarget code? Yes, still Bitcoin., Is there a minority hashrate branch of the chain? Not Bitcoin. Change the proof-of-work? Not Bitcoin. Majority hashrate decides 1% inflation a year is a Good Idea? Not Bitcoin., Is there a better technical definition of what should or shouldn’t be considered “Bitcoin” ?, Apologies if I’m accidentally stealing this from somebody, it seems like the obvious definition given Satoshi’s original whitepaper and implementation. ↩
Perhaps the most common objection I hear to raising the maximum block size from one megabyte is that “blocks aren’t full yet, so we don’t have to do anything (yet).”, It is true that we’re not yet at the hard-coded one megabyte block size limit; on average, blocks are 30-40% full today., There is a very good blog post by David Hudson at hashingit.com analyzing what will happen on the network as we approach 100% full blocks. Please visit that link for full details, but basically he points out there is a mismatch between when transactions are created and when blocks are found– and that mismatch means very bad things start to happen on the network as the one megabyte limit is reached., Transactions are created steadily over time, as people spend their bitcoin. There are daily and weekly cycles in transaction volume, but over any ten-minute period the number of transactions will be roughly equal to the number of transactions in the previous or next ten minute period., Blocks, however, are created via a random Poisson process. Sometimes a lot of blocks are found in an hour, sometimes all the miners will be unlucky and very few (or none!) will be found in a hour., The mismatch between the steady submission of transactions to the network and the random Poisson distribution of found blocks means we will never have blocks that are 100% full all of the time. Sometimes miners will find a lot of blocks in a row, clearing out the queue of waiting transactions., Conversely, very bad things can happen when miners happen to be unlucky. The queue of transactions waiting to be confirmed will grow, using more and more memory inside every full node. Full nodes could (and probably will in a future release of Bitcoin Core) start to drop transactions from the queue, which will make transaction confirmation less reliable., If the wallet re-broadcasts transactions if they are not confirmed after a few blocks (the Bitcoin Core wallet does), then bandwidth usage spikes as every wallet on the network rebroadcasts its unconfirmed transactions., If the number of transactions waiting gets large enough, the end result will be an over-saturated network, busy doing nothing productive. I don’t think that is likely– it is more likely people just stop using Bitcoin because transaction confirmation becomes increasingly unreliable.
Get ten engineers on a mailing list, ask them to solve a big problem, and you’ll probably end up with eleven different solutions., Even if people agree that the one megabyte block size limit should be raised (and almost everybody does agree that it should be raised at some point), agreeing how is difficult., I’m not going to try to list all of the proposals for how to increase the size; there are too many of them, and I’d just manage to miss somebody’s favorite (and end up with a wall-of-text blog post that nobody would read). But I will write about one popular family of ideas, and will explain the reasoning behind the twenty-megabyte proposal., One very popular idea is to implement a dynamic limit, based on historical block sizes., The details vary: how often should the maximum size be adjusted? Every block? Every difficulty adjustment? How much of an increase should be allowed? 50% bigger? Double?, If the block size limit is just a denial-of-service prevention measure (preventing a big, evil miner from producing an 11 gigabyte block that the rest of the network is forced to validate), then any of these proposals will work. Engineers could bike-shed the parameter choice to death, but I like the idea of a simple dynamic limit on the maximum allowed size., There are more complicated proposals for a dynamic block size limit that (for example) involve proof-of-stake voting or linking the maximum block size to the mining reward and/or the amount of fees in a block. I like them less than a simple solution, because consensus-critical code must be absolutely correct, and every additional line of code is another opportunity for a chain-splitting bug to slip through code review and testing., Some of the core committers don’t like the idea of giving miners the ability to collude (either intentionally or as a response to economic incentives) to increase the block size without limit., For example, Gregory Maxwell wrote:, Do people (other than Mike Hearn; I guess) think a future where everyone depends on a small number of “Google scale” node operations for the system is actually okay? (I think not, and if so we’re never going to agree–but it can be helpful to understand when a disagreement is ideological)., Greg really should have said “other than Mike Hearn and Satoshi”:, Long before the network gets anywhere near as large as that, it would be safe for users to use Simplified Payment Verification (section 8) to check for double spending, which only requires having the chain of block headers, or about 12KB per day.  Only people trying to create new coins would need to run network nodes. At first, most users would run network nodes, but as the network grows beyond a certain point, it would be left more and more to specialists with server farms of specialized hardware., I struggle with wanting to stay true to Satoshi’s original vision of Bitcoin as a system that scales up to Visa-level transaction volume versus maintaining consensus with the other core committers, who obviously have a different vision for how the system should grow. Twenty megabytes is meant to be a compromise– large enough to support transaction volume for the next couple of years, but small enough to make sure volunteer open source developers can continue to process the entire chain on their home Internet connection or on a modest virtual private server., If compromise isn’t possible, then a simple dynamic limit intended just to prevent DoS attacks is a very attractive long-term solution.
Now that six months have gone past, I’m being asked if I still think Craig Wright was Satoshi., I think there are two possibilities., Either he was Satoshi, but really wants the world to think he isn’t, so he created an impossible-to-untangle web of truths, half-truths and lies. And ruined his reputation in the process., If he was Satoshi, we should respect his wish to remain anonymous, and ignore him., The other possibility is he is a master scammer/fraudster who managed to trick some pretty smart people over a period of several years., In which case everybody except the victims of his fraud and law enforcement working on behalf of those victims should ignore him., So, either he was or he wasn’t. In either case, we should ignore him. I regret ever getting involved in the “who was Satoshi” game, and am going to spend my time on more fun and productive pursuits.
Increasing the block size limit from 1 million bytes to 2 million bytes sounds so simple:  just change the “1” to a “2” in the source code and we’re done, right?, If we didn’t care about a smooth upgrade, then it could be that simple. Just change this line of code (in src/consensus/consensus.h):, to:, If you make that one-byte change then recompile and run Bitcoin Core, it will work. You computer will download the blockchain and will interoperate with the other computers on the network with no issues., If your computer is assembling transactions into blocks (you are a solo miner or a mining pool operator), then things get more complicated. I’ll walk through that complexity in the rest of this blog post, hopefully giving you a flavor for the time and care put into making sure consensus-level changes are safe. , Github has a handy feature to compare code side-by-side; go to https://github.com/bitcoin/bitcoin/compare/v0.11.2…gavinandresen:two_mb_bump to see just the code changes for the two-megabyte fork. You should see something like this:, , There are five “commits” – groups of code changes that go together– that implement the limit increase (ignore the first commit by David Harding, that’s just the last commit in the Bitcoin Core 0.11.2 release)., Twenty-two files changed, with about 900 new lines of code – more than half of which (500 lines) are new tests to make sure the new code works properly., The first commit is “Minimal consensus/miner changes for 2mb block size bump”, and is small – under twenty lines of new code. You can see the one-line change of MAX_BLOCK_SIZE from 1,000,000 bytes to 2,000,000 bytes; the rest of the changes are needed by miners so they know whether it is safe to produce bigger blocks. A new MaxBlockSize() method is defined that returns either the old or new maximum block size, depending on the timestamp in the 80-byte block header. I might write an entire blog post about why it uses the timestamp instead of the block height or the median time of the last eleven blocks or something else… but not today., The consensus change is on line 2816 of main.cpp in the CheckBlock() method, and just uses the new MaxBlockSize() method to decide whether or not a block is too big instead of MAX_BLOCK_SIZE. A similar change is made to the CreateNewBlock() function in miner.cpp and the ‘getblocktemplate’ remote procedure call so miners can create bigger blocks., The next commit (“Testing infrastructure fixes”) adds a couple of features and fixes a bug in the code we use to test changes to the Bitcoin code., There are two levels of code-testing-code; unit tests are put into the tree at src/test/, are written in C++, and very low level, testing to make sure various pieces of code behave properly. There are also regression tests in the tree at qa/rpc-tests/. They are written in Python and use the RPC (remote procedure call) interface to the command-line bitcoind running in “-regtest mode” to make sure everything is working properly., “Two megabyte fork after miner vote and grace period” is by far the biggest of the commits– almost 700 lines of new code. It implements the roll-out rules: only allow bigger blocks 28 days after 75% of hashpower has shown support by producing blocks with a special bit set in the block version number., 75% and 28-days are fairly arbitrary choices. I hate arbitrary choices, mostly because it is so easy for everybody to have an opinion about them (also known as “bikeshedding”) and to spend days in unfruitful argument.  I’ve written another blog post explaining why we think those are good numbers to choose., The miner-vote-and-grace-period code comes from the BIP 101 implementation I did for Bitcoin XT, and has three levels of tests., There is a new unit test in block_size_tests.cpp. It tests the CheckBlock() call, creating blocks that are exactly at the old or new size limits, or exactly one byte bigger than the old or new size limits, and testing to make sure they are or are not accepted based on whether their timestamps are before or after (or exactly at) the time when larger blocks are allowed., There is also a new regression test, bigblocks.py. It runs four copies of bitcoind, creates a block-chain just for testing on the developer’s machine (in -regtest mode blocks can be created instantly) and then tests the fork activation code, making sure that miner votes are counted correctly, that blockchain re-organizations are handled properly, that bigger blocks will not be created until after the grace period is over. It also makes sure this code will report itself as obsolete if 75% of hashing power does not adopt the change before the January, 2018 deadline., The vast majority of my development time was spent making sure that the regression and unit tests thoroughly exercised the new code. Then once the regression and unit tests pass, more testing is done on the test network across the Internet. Writing the code is the easy part., And finally, this rollout code was extensively tested by Jonathan Toomim when he tested the eight megabyte blocks and Bitcoin XT on the testnet across the Internet (and across the great firewall of China)., Continuing the exploration of the code changes…, There are changes to the IsSuperMajority() function in main.cpp and a new VersionKnown() function in block.h that are used to count up the number of blocks that support various changes. The complexity here is to be compatible with BIP 009 (“version bits”) and various soft forks BIPs (68, 112, 113, 141) that might happen at the same time as the block size limit increase., The largest number of new lines of code (besides tests) is to txdb.cpp. When the miner vote succeeds, the hash of the triggering block is written to the block chain index. That isn’t strictly necessary– the code could scan all the headers in the block chain to figure out if the vote succeeded every time it started up. It is just much more efficient to store that piece of information in the block chain index database., Phew. Describing all that probably took longer than writing the code. Two more commits to go., “Accurate sigop/sighash accounting and limits” is important, because without it, increasing the block size limit might be dangerous. You can watch my presentation at DevCore last November for the details, but basically Satoshi didn’t think hard enough about how transactions are signed, and as a result it is possible to create very large transactions that are very expensive to validate. This commit cleans up some of that “technical debt,” implementing a new ValidationCostTracker that keeps track of how much work is done to validate transactions and then uses it along with a new limit (MAX_BLOCK_SIGHASH) to make sure nobody can create a very expensive-to-validate block to try to gum up the network., There are strong incentives for people not to create expensive-to-validate blocks (miners want their blocks to get across the network as quickly as possible, and work hard on that to minimize the ‘orphan rate’ for their blocks). But one of the principles of secure coding is “belts and suspenders” (“defense in depth” if you want to sound professional or don’t like suspenders)., MAX_BLOCK_SIGHASH is another annoying, arbitrary limit. It is set to 1.3 gigabytes, which is big enough so none of the blocks currently in the block chain would hit it, but small enough to make it impossible to create poison blocks that take minutes to validate., The final commit, “Do not relay or mine excessive sighash transactions”, is another belt-and-suspenders safety measure. There were already limits in place to reject very large, expensive-to-validate transactions, but this commit adds another check to make absolutely sure a clever attacker can’t trick a miner into putting a crazy-expensive transaction into their blocks., If you paid attention all the way to here, you have a longer attention span than me. For the non-programmers reading this, I hope I’ve given you some insight into the care and level of thought that goes into changing that “1” to a “2”.
Pieter Wuille gave a fantastic presentation on “Segregated Witness” in Hong Kong. It’s a great idea, and should be rolled into Bitcoin as soon as safely possible. It is the kind of fundamental idea that will have huge benefits in the future. It also needs a better name (“segregated” has all sorts of negative connotations…)., You should watch Pieter’s presentation, but I’ll give a different spin on explaining what it is (I know I often need something explained to me a couple different ways before I really understand it)., So… sending bitcoin into a segregate witness-locked output will look like a weird little beastie in today’s blockchain explorers– it will look like an “anyone can spend” transaction, with a scriptPubKey of:, PUSHDATA [version_byte + validation_script], Spends of segregated witness-locked outputs will have a completely empty scriptSig., The reason that is not insane is because the REAL scriptSig for the transaction will be put in a separate, new data structure, and wallets and miners that are doing validation will use that new data structure to make sure the signatures for the transaction are valid, etc., That data structure will be a merkle tree that mirrors the transaction merkle tree that is put into the block header of every block. Every transaction with a segregated witness input will have an entry in that second merkle tree with the signature data in it (plus 10 or so extra bytes per input to enable fraud proofs)., The best design is to combine the transaction and segregated witness merkle trees into one tree, with the left side of the tree being the transaction  data and the right side the segregated witness data. The merkle root in the block header would just be that combined tree. That could (and should, in my opinion) be done as a hard fork; Pieter proposes doing it as a soft fork, by stuffing the segregated witness merkle root into the first (coinbase) transaction in each block, which is more complicated and less elegant but means it can be rolled out as a soft fork., Regardless of how it is rolled out, it would be a smooth transition for wallets and most end-users– if you don’t want to use newfangled segregated witness transactions, you don’t have to.  Paying to somebody who is using the newfangled transactions looks just like paying to somebody using a newfangled multisig wallet (a ‘3something’ BIP13 bitcoin address)., There is no requirement that wallets upgrade, but anybody generating a lot of transactions will have a strong incentive to produce segregated witness transactions– Pieter proposes to give segregated witness transactions a discount on transaction fees, by not completely counting the segregated witness data when figuring out the fee-per-kilobyte transaction charge., So… how does all of this help with the one megabyte block size limit?, Well, once all the details are worked out, and the soft or hard fork is past, and a significant fraction of transactions are spending segregated witness-locked outputs… more transactions will fit into the 1 megabyte hard limit.  For example, the simplest possible one-input, one-output segregated witness transaction would be about 90 bytes of transaction data plus 80 or so bytes of signature– only those 90 bytes need to squeeze into the one megabyte block, instead of 170 bytes. More complicated multi-signature transactions save even more., So once everybody has moved their coins to segregated witness-locked outputs and all transactions are using segregated witness, two or three times as many transactions would squeeze into the one megabyte block limit., Segregated witness transactions won’t help with the current scaling bottleneck, which is how long it takes a one-megabyte ‘block’ message to propagate across the network– they will take just as much bandwidth as before. There are several projects in progress to try to fix that problem (IBLTs, weak blocks, thin blocks, a “blocktorrent” protocol) and one that is already deployed and making one megabyte block propagation much faster than it would otherwise be (Matt Corallo’s fast relay network)., I think it is wise to design for success. Segregated witness is cool, but it isn’t a short-term (within the next six months to a year) solution to the problems we’re already seeing as we run into the one-megabyte block size limit.
